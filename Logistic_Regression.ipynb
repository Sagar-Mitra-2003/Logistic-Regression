{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression | Assignment"
      ],
      "metadata": {
        "id": "Tsr-CLyzQJAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Answer :\n",
        "***\n",
        "Logistic regression is a supervised machine learning algorithm primarily used for classification tasks, not regression, to predict the probability of an event occurring or an instance belonging to a particular class. It models the relationship between independent variables and a categorical dependent variable, using the S-shaped sigmoid function to transform the output into a probability between 0 and 1. This probability is then used with a threshold (commonly 0.5) to classify data points into discrete categories, such as \"spam\" or \"not spam\" for emails, or \"disease present\" versus \"disease absent\".  \n",
        "\n",
        "Logistic Regression predicts the probability of a categorical outcome, often binary (like yes/no, 0/1), using an S-shaped sigmoid curve, whereas Linear Regression predicts a continuous numerical outcome (like price or temperature) by fitting a straight line to the data. The core difference lies in their target variable type: logistic regression is for classification and linear regression is for regression, meaning linear regression finds a continuous value, and logistic regression predicts a probability that falls into a discrete category.\n",
        "***"
      ],
      "metadata": {
        "id": "Hz4wAr0JQJ_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 : Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "Answer :\n",
        "***\n",
        "The Sigmoid function, or logistic function, is crucial in Logistic Regression because it transforms the raw output of a linear model into a probability value between 0 and 1. This S-shaped function \"squashes\" any real-valued input into this bounded range, making the output directly interpretable as the probability of a binary event occurring (e.g., 0.88 meaning an 88% chance). This probabilistic output is essential for binary classification, enabling the model to predict which of two classes an input belongs to based on a set threshold.\n",
        "\n",
        "Here's a breakdown of its role:\n",
        "1. Probability Mapping:\n",
        "Logistic regression starts by calculating a linear combination of input features, which can produce any real number. The sigmoid function takes this raw score and maps it to a value between 0 and 1, which is a valid probability.\n",
        "2. Binary Classification:\n",
        "The model uses this probability to classify an input into one of two categories. For example, if the probability exceeds a certain threshold (often 0.5), the input is classified as belonging to the positive class; otherwise, it's assigned to the negative class.\n",
        "3. Interpretability:\n",
        "Without the sigmoid function, the output of the linear model wouldn't be directly interpretable as a probability. The sigmoid function provides this essential probabilistic interpretation, which is valuable for risk assessment and other predictive tasks.\n",
        "4. Log-Odds Connection:\n",
        "Mathematically, the output of the linear model (before the sigmoid) can be seen as the \"log-odds\" of the event, and the sigmoid function is derived from this relationship, making the connection explicit and providing a well-defined link between the model's linear component and the predicted probability.\n",
        "***"
      ],
      "metadata": {
        "id": "rP6yfMgnQ_3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3 : What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "Answer :\n",
        "***\n",
        "Regularization in Logistic Regression is a technique that adds a penalty to the model's complexity, preventing it from overfitting the training data. It's needed because logistic regression, especially with many features, can learn complex patterns in the training data that don't generalize to new, unseen data, leading to poor predictive performance. By penalizing large model coefficients, regularization encourages simpler models that are more robust and accurate on new data.\n",
        "\n",
        "Characteristics of Regularization :-\n",
        "1. Penalty for Complexity:\n",
        "Regularization introduces a penalty term into the logistic regression model's objective function (the function it tries to minimize).\n",
        "2. Shrinks Coefficients:\n",
        "This penalty discourages excessively large coefficients for the input features, which would make the model too complex.\n",
        "3. Trade-off:\n",
        "It creates a trade-off between fitting the training data perfectly (low training error) and building a model that generalizes well to unseen data (low generalization error).\n",
        "\n",
        "Need of Regularization :-\n",
        "1. To Prevent Overfitting:\n",
        "The primary reason for regularization is to combat overfitting, a common problem where a model learns the noise and specific details of the training data too well, leading to poor performance on new data.\n",
        "2. High-Dimensional Data:\n",
        "Logistic regression models with many features are particularly prone to overfitting.\n",
        "3. Improve Generalization:\n",
        "By controlling model complexity, regularization helps the model generalize better to new, unseen data, resulting in more reliable predictions.\n",
        "4. Reduce High Variance:\n",
        "Overfitting often manifests as high variance, meaning the model is highly sensitive to the specific training data. Regularization reduces this variance by increasing the model's bias, leading to a more stable and generalized model.\n",
        "\n",
        "Common Regularization Techniques :-\n",
        "1. L1 Regularization (Lasso):\n",
        "Adds a penalty equal to the absolute value of the coefficients. It can shrink some coefficients to exactly zero, effectively performing feature selection and creating sparse models.\n",
        "2. L2 Regularization (Ridge):\n",
        "Adds a penalty equal to the square of the coefficients. It shrinks coefficients towards zero but rarely to exactly zero, distributing the importance across features rather than eliminating them.\n",
        "***"
      ],
      "metadata": {
        "id": "CCJC2hNgReXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4 : What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        "Answer :\n",
        "***\n",
        "Common classification evaluation metrics include Accuracy, Precision, Recall (Sensitivity), F1-Score, Specificity, and the Confusion Matrix, which are crucial for understanding a model's performance beyond simple correct/incorrect counts. These metrics are important because the ideal metric depends on the specific task's costs of different errors—such as missing a disease (false negative) vs. falsely diagnosing one (false positive)—and the balance of the dataset's classes.\n",
        "\n",
        "Common Classification Metrics\n",
        "1. Confusion Matrix:\n",
        "A table that breaks down a model's predictions into four components: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). It forms the basis for other metrics.\n",
        "2. Accuracy:\n",
        "The proportion of all predictions that were correct (TP+TN) out of the total number of predictions. It is useful for balanced datasets but can be misleading with imbalanced ones.\n",
        "3. Precision:\n",
        "The proportion of positive predictions that were actually correct (TP / (TP + FP)). High precision means that when the model predicts a positive class, it is usually correct, which is important to minimize false alarms.\n",
        "4. Recall (Sensitivity or True Positive Rate):\n",
        "The proportion of actual positive instances that were correctly identified (TP / (TP + FN)). High recall indicates the model's ability to find all positive cases and is crucial for tasks like medical diagnosis where false negatives have severe consequences.\n",
        "5. F1-Score:\n",
        "The harmonic mean of precision and recall (2 * (Precision * Recall) / (Precision + Recall)). It provides a single metric that balances both precision and recall, making it useful for imbalanced datasets where you want both to be high.\n",
        "6. Specificity (True Negative Rate):\n",
        "The proportion of actual negative instances that were correctly identified (TN / (TN + FP)). It measures the model's ability to correctly identify negatives.\n",
        "7. Area Under the ROC Curve (AUC-ROC):\n",
        "Measures the model's ability to distinguish between positive and negative classes across various probability thresholds. It is a good metric for evaluating models that output probabilities.\n",
        "\n",
        "Why They Are Important\n",
        "1. Task-Specific Decision Making:\n",
        "The choice of metric must align with the real-world problem's objectives. For example, prioritizing Recall is critical in disease screening to avoid missing cases (false negatives), while Precision is vital in spam detection to avoid marking legitimate emails as spam (false positives).\n",
        "2. Handling Imbalanced Datasets:\n",
        "Accuracy can be a poor indicator when one class is significantly more common than others. Metrics like Precision, Recall, and F1-Score provide a more nuanced view of the model's performance on minority classes, which might be the more critical ones.\n",
        "3. Understanding Trade-offs:\n",
        "Metrics like Precision and Recall represent different types of errors. Using them helps in understanding the trade-offs inherent in the model's predictions and choosing a threshold that best fits the specific business or scientific goals.\n",
        "4. Comprehensive Evaluation:\n",
        "No single metric tells the whole story. A combination of metrics from the confusion matrix provides a comprehensive picture of a model's strengths and weaknesses, leading to better model development and selection.\n",
        "***"
      ],
      "metadata": {
        "id": "Y257P5-CS9QA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5 : Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "Answer :-\n",
        "***"
      ],
      "metadata": {
        "id": "LdmvZj2NhpwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=5000)  # increased max_iter for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQjRmCZwh-WN",
        "outputId": "12a911e2-e168-4e81-bdba-d9a4d1a52de1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "_bmYZQuai3gV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "Answer :\n",
        "***"
      ],
      "metadata": {
        "id": "eNjpUpMMi6o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Logistic Regression with L2 regularization (Ridge is default)\n",
        "model = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print coefficients and accuracy\n",
        "print(\"Model Coefficients (per feature):\")\n",
        "for feature, coef in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "print(\"\\nIntercept:\", model.intercept_[0])\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0YCDx2Vjs8W",
        "outputId": "8a512ffd-89ac-41f0-eb19-b27bf7b6998f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients (per feature):\n",
            "mean radius: 0.8071\n",
            "mean texture: 0.1133\n",
            "mean perimeter: -0.2831\n",
            "mean area: 0.0252\n",
            "mean smoothness: -0.1673\n",
            "mean compactness: -0.2022\n",
            "mean concavity: -0.4551\n",
            "mean concave points: -0.2524\n",
            "mean symmetry: -0.3092\n",
            "mean fractal dimension: -0.0312\n",
            "radius error: -0.0551\n",
            "texture error: 1.1033\n",
            "perimeter error: 0.0856\n",
            "area error: -0.0960\n",
            "smoothness error: -0.0223\n",
            "compactness error: 0.0591\n",
            "concavity error: -0.0214\n",
            "concave points error: -0.0354\n",
            "symmetry error: -0.0404\n",
            "fractal dimension error: 0.0137\n",
            "worst radius: 0.0952\n",
            "worst texture: -0.3769\n",
            "worst perimeter: -0.0878\n",
            "worst area: -0.0146\n",
            "worst smoothness: -0.3248\n",
            "worst compactness: -0.7477\n",
            "worst concavity: -1.3233\n",
            "worst concave points: -0.5634\n",
            "worst symmetry: -0.7879\n",
            "worst fractal dimension: -0.0916\n",
            "\n",
            "Intercept: 29.173300070114156\n",
            "Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "lGfnXR09kAbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7 : Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "Answer :\n",
        "***\n"
      ],
      "metadata": {
        "id": "LM8nd67NkCnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Logistic Regression with OvR\n",
        "model = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuaGhbTDkRpe",
        "outputId": "30fe4ca6-d744-4970-b72c-d4f0aa3fd8e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.80      0.89        10\n",
            "   virginica       0.83      1.00      0.91        10\n",
            "\n",
            "    accuracy                           0.93        30\n",
            "   macro avg       0.94      0.93      0.93        30\n",
            "weighted avg       0.94      0.93      0.93        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "VntFyY5FlCh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8 : Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "Answer:\n",
        "***"
      ],
      "metadata": {
        "id": "tYUHH9AjlD3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    \"C\": [0.01, 0.1, 1, 10, 100],            # Regularization strength\n",
        "    \"penalty\": [\"l1\", \"l2\"],                 # Penalty type\n",
        "    \"solver\": [\"liblinear\"]                  # Solver that supports both l1 & l2\n",
        "}\n",
        "\n",
        "# Logistic Regression + GridSearchCV\n",
        "grid = GridSearchCV(\n",
        "    LogisticRegression(max_iter=5000),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and validation accuracy\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY4xtzA2ndRH",
        "outputId": "478f056a-ead9-41cf-ad0f-5953b10d4a7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Cross-Validation Accuracy: 0.9666666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "AG-fJfUcnuPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9 : Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "Answer :\n",
        "***"
      ],
      "metadata": {
        "id": "qXttwt9znwEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Without Scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=5000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# With Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaling = LogisticRegression(max_iter=5000)\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "acc_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "# Compare Results\n",
        "print(\"Accuracy without Scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with Scaling:\", acc_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrUvrknAn9QH",
        "outputId": "315e71d0-d670-42a9-b961-681b1c0c488b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.9649122807017544\n",
            "Accuracy with Scaling: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "TwlA8mb-oXXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10 : Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.\n",
        "\n",
        "Answer :\n",
        "***"
      ],
      "metadata": {
        "id": "QQ3li3d7oYk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s how I’d approach building a Logistic Regression model for predicting customer responses to a marketing campaign, step by step :\n",
        "\n",
        "1. Understand the Problem\n",
        "\n",
        "* Target variable : Response (1 = responded, 0 = no response)\n",
        "* Challenge : Severe class imbalance (only \\~5% positives).\n",
        "* Goal : Build a model that identifies responders well (business cares about not missing responders, but also avoiding spamming non-responders).\n",
        "\n",
        "2. Data Preparation :\n",
        "\n",
        "* Explore Data : Check missing values, outliers, skewed distributions.\n",
        "* Feature Engineering :\n",
        "\n",
        "  * Customer demographics (age, income, location).\n",
        "  * Purchase history (frequency, recency, monetary value).\n",
        "  * Marketing engagement (email opens, clicks).\n",
        "\n",
        "3. Feature Scaling :\n",
        "\n",
        "* Logistic Regression is sensitive to feature scales (since it uses regularization).\n",
        "* Apply StandardScaler (z-score normalization) to continuous features.\n",
        "* Leave categorical variables encoded (via One-Hot Encoding or Target Encoding).\n",
        "\n",
        "4. Handling Imbalanced Classes\n",
        "\n",
        "Several strategies:\n",
        "\n",
        "1. Class Weights:\n",
        "\n",
        "   * In `LogisticRegression(class_weight=\"balanced\")`, the algorithm gives more importance to the minority class.\n",
        "2. Resampling:\n",
        "\n",
        "   * Oversample responders (SMOTE) or undersample non-responders.\n",
        "   * Use only on training set (never test set).\n",
        "3. Hybrid approach: Start with class weights, then experiment with SMOTE.\n",
        "\n",
        "5. Model Training & Hyperparameter Tuning\n",
        "\n",
        "* Base model: Logistic Regression.\n",
        "* Important hyperparameters:\n",
        "\n",
        "  * `C`: Regularization strength.\n",
        "  * `penalty`: L1 (feature selection) vs. L2 (ridge).\n",
        "  * `solver`: e.g., `liblinear`, `saga`.\n",
        "* Use GridSearchCV or RandomizedSearchCV with Stratified K-Fold CV (to preserve imbalance in splits).\n",
        "\n",
        "6. Evaluation Metrics\n",
        "\n",
        "* Accuracy is misleading (model could get 95% accuracy by predicting all 0s).\n",
        "* Instead, focus on:\n",
        "\n",
        "  * Precision & Recall (for responders)\n",
        "  * F1-score (balance between precision & recall)\n",
        "  * ROC-AUC (overall ranking ability).\n",
        "  * PR-AUC (Precision-Recall curve) (better for highly imbalanced data).\n",
        "\n",
        "Business perspective:\n",
        "\n",
        "* If cost of spamming non-responders is low → prioritize **Recall** (catch more responders).\n",
        "* If cost of spamming is high → prioritize **Precision**.\n",
        "\n",
        "7. Final Steps\n",
        "\n",
        "* Train final model with best hyperparameters.\n",
        "* Calibrate predicted probabilities (using Platt scaling or isotonic regression) if probability estimates are needed for ranking customers.\n",
        "* Deploy model and monitor performance (check drift, recalibrate periodically).\n",
        "\n",
        "Summary of Approach:\n",
        "\n",
        "1. Clean & preprocess features (scale numerics, encode categoricals).\n",
        "2. Handle imbalance using class\\_weight=\"balanced\" and/or SMOTE.\n",
        "3. Train Logistic Regression with hyperparameter tuning (C, penalty).\n",
        "4. Evaluate using Precision, Recall, F1, ROC-AUC, PR-AUC (not just accuracy).\n",
        "5. Align metric choice with business objective (catch more responders vs. reduce false positives).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zymTZJiiqGvI"
      }
    }
  ]
}